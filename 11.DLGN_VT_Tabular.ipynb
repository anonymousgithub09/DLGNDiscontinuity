{"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"00ad5f1807eee938f7727b558c9158a01118eae9a3a444b82c1137c2e4c2794d"}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#@title Imports\n%reset -f ","metadata":{"id":"kjNxUX51Lh0k","execution":{"iopub.status.busy":"2024-03-13T20:24:04.204306Z","iopub.execute_input":"2024-03-13T20:24:04.204760Z","iopub.status.idle":"2024-03-13T20:24:04.672413Z","shell.execute_reply.started":"2024-03-13T20:24:04.204724Z","shell.execute_reply":"2024-03-13T20:24:04.671500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.674260Z","iopub.execute_input":"2024-03-13T20:24:04.674596Z","iopub.status.idle":"2024-03-13T20:24:04.679937Z","shell.execute_reply.started":"2024-03-13T20:24:04.674557Z","shell.execute_reply":"2024-03-13T20:24:04.678975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pylab\nimport scipy.io","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.681017Z","iopub.execute_input":"2024-03-13T20:24:04.681370Z","iopub.status.idle":"2024-03-13T20:24:04.690035Z","shell.execute_reply.started":"2024-03-13T20:24:04.681335Z","shell.execute_reply":"2024-03-13T20:24:04.689204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport shutil\nimport numpy as np\nfrom itertools import product as cartesian_prod\n\nimport matplotlib.pyplot as plt\n\nimport urllib.request\nfrom scipy.io import arff\nfrom copy import deepcopy\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport os\nimport argparse\nimport sys\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nnp.set_printoptions(precision=2)\n\ndef set_npseed(seed):\n    np.random.seed(seed)\ndef set_torchseed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\ndef sigmoid(u):\n    u = np.maximum(u,-100)\n    u = np.minimum(u,100)\n    return 1/(1+np.exp(-u))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.692540Z","iopub.execute_input":"2024-03-13T20:24:04.693306Z","iopub.status.idle":"2024-03-13T20:24:04.703512Z","shell.execute_reply.started":"2024-03-13T20:24:04.693277Z","shell.execute_reply":"2024-03-13T20:24:04.702656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import pairwise_distances","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.704716Z","iopub.execute_input":"2024-03-13T20:24:04.705097Z","iopub.status.idle":"2024-03-13T20:24:04.717393Z","shell.execute_reply.started":"2024-03-13T20:24:04.705063Z","shell.execute_reply":"2024-03-13T20:24:04.716477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Args:\n    def __init__(self):\n        self.numlayer=3\n        self.numnodes=5\n        self.beta=5.\n        self.lr=.1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.718664Z","iopub.execute_input":"2024-03-13T20:24:04.718963Z","iopub.status.idle":"2024-03-13T20:24:04.726118Z","shell.execute_reply.started":"2024-03-13T20:24:04.718938Z","shell.execute_reply":"2024-03-13T20:24:04.725267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.727332Z","iopub.execute_input":"2024-03-13T20:24:04.727652Z","iopub.status.idle":"2024-03-13T20:24:04.737564Z","shell.execute_reply.started":"2024-03-13T20:24:04.727626Z","shell.execute_reply":"2024-03-13T20:24:04.736520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DLGN_FC(nn.Module):\n    def __init__(self, input_dim=None, output_dim=None, num_hidden_nodes=[], beta=30, mode='pwc'):\t\t\n        super(DLGN_FC, self).__init__()\n        self.num_hidden_layers = len(num_hidden_nodes)\n        self.beta=beta  # Soft gating parameter\n        self.mode = mode\n        self.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n        self.gating_layers=nn.ModuleList()\n        self.value_layers=nn.Parameter(torch.randn([1]+num_hidden_nodes)/100.) #[1, 12, 12, 12, 12]\n        self.num_layer = len(num_hidden_nodes)\n        self.num_hidden_nodes = num_hidden_nodes\n        for i in range(self.num_hidden_layers+1):\n            if i!=self.num_hidden_layers:\n                temp = nn.Linear(self.num_nodes[0], self.num_nodes[i+1], bias=False)\n                self.gating_layers.append(temp)\n\n    def set_parameters_with_mask(self, to_copy, parameter_masks):\n        # self and to_copy are DLGN_FC objects with same architecture\n        # parameter_masks is compatible with dict(to_copy.named_parameters())\n        for (name, copy_param) in to_copy.named_parameters():\n            copy_param = copy_param.clone().detach()\n            orig_param  = self.state_dict()[name]\n            if name in parameter_masks:\n                param_mask = parameter_masks[name]>0\n                orig_param[param_mask] = copy_param[param_mask]\n            else:\n                orig_param = copy_param.data.detach()\n\n    def return_gating_functions(self):\n        effective_weights = []\n        for i in range(self.num_hidden_layers):\n            curr_weight = self.gating_layers[i].weight.detach().clone()\n            # curr_weight /= torch.norm(curr_weight, dim=1, keepdim=True)\n            effective_weights.append(curr_weight)\n        return effective_weights\n        # effective_weights (and effective biases) is a list of size num_hidden_layers\n\n\n    def forward(self, x):\n        for el in self.parameters():\n            if el.is_cuda:\n                device = torch.device('cuda')\n            else:\n                device = torch.device('cpu')\n        values=[torch.ones(x.shape).to(device)]\n        for i in range(self.num_hidden_layers):\n            fiber = [len(x)]+[1]*self.num_layer\n#             print(\"fiber:\",fiber)\n            fiber[i+1] = self.num_hidden_nodes[i]\n#             print(\"fiber:\",fiber)\n            fiber = tuple(fiber)\n#             print(\"fiber:\",fiber)\n            gate_score = torch.sigmoid( self.beta*(x@self.gating_layers[i].weight.T))#/\n                #   torch.norm(self.gating_layers[i].weight, dim=1, keepdim=True).T) \n#             print(\"gate_score:\",gate_score.shape)\n            gate_score = gate_score.reshape(fiber) \n#             print(\"gate_score:\",gate_score.shape)\n            if i==0:\n                cp = gate_score\n#                 print(\"cp:\",cp.shape)\n            else:\n                cp = cp*gate_score \n#                 print(\"cp:\",cp.shape)\n#             print(\"return:\",torch.sum(cp*self.value_layers, dim=(1,2,3,4)).shape)\n        return torch.sum(cp*self.value_layers, dim=(1,2,3))","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.738962Z","iopub.execute_input":"2024-03-13T20:24:04.739250Z","iopub.status.idle":"2024-03-13T20:24:04.756898Z","shell.execute_reply.started":"2024-03-13T20:24:04.739226Z","shell.execute_reply":"2024-03-13T20:24:04.756016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Train DLGN model\ndef train_dlgn (DLGN_obj, train_data_curr,vali_data_curr,test_data_curr,\n                train_labels_curr,test_labels_curr,vali_labels_curr,\n                parameter_mask=dict()):\n    # DLGN_obj is the initial network\n    # parameter_mask is a dictionary compatible with dict(DLGN_obj.named_parameters())\n    # if a key corresponding to a named_parameter is not present it is assumed to be all ones (i.e it will be updated)\n\n    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n\n    # Speed up of a factor of over 40 by using GPU instead of CPU\n    # Final train loss of 0.02 and test acc of 74%\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # device = torch.device('cpu')\n    DLGN_obj.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n\n\n\n    optimizer = optim.SGD(DLGN_obj.parameters(), lr=lr)\n\n\n\n    train_data_torch = torch.Tensor(train_data_curr)\n    vali_data_torch = torch.Tensor(vali_data_curr)\n    test_data_torch = torch.Tensor(test_data_curr)\n\n    train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n    test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n    vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n\n    num_batches = no_of_batches\n    batch_size = len(train_data_curr)//num_batches\n    losses=[]\n    DLGN_obj_store = []\n    best_vali_error = len(vali_labels_curr)\n\n\n    # print(\"H3\")\n    # print(DLGN_params)\n    debug_models= []\n    train_losses = []\n    tepoch = tqdm(range(saved_epochs[-1]+1))\n    for epoch in tepoch:  # loop over the dataset multiple times\n        if epoch in update_value_epochs:\n            # updating the value pathdim vector by optimising \n\n            train_preds =DLGN_obj(torch.Tensor(train_data_curr).to(device)).reshape((-1,1))\n            criterion = nn.CrossEntropyLoss()\n            outputs = torch.cat((-1*train_preds,train_preds), dim=1)\n            targets = torch.tensor(train_labels_curr, dtype=torch.int64).to(device)\n\n            train_loss = criterion(outputs, targets)\n            print(\"Loss before updating value_net at epoch\", epoch, \" is \", train_loss)\n            print(\"Total path abs value\", torch.abs(DLGN_obj.value_layers.cpu().detach()).sum().numpy())\n\n            ew = DLGN_obj.return_gating_functions()\n            cp_feat1 = sigmoid(beta*np.dot(train_data_curr,ew[0].cpu().T).reshape(-1,num_neuron,1,1))\n#             print(\"cp_feat1:\",cp_feat1.shape)\n            cp_feat2 = sigmoid(beta*np.dot(train_data_curr,ew[1].cpu().T).reshape(-1,1,num_neuron,1))\n#             print(\"cp_feat2:\",cp_feat2.shape)\n            cp_feat3 = sigmoid(beta*np.dot(train_data_curr,ew[2].cpu().T).reshape(-1,1,1,num_neuron))\n#             print(\"cp_feat3:\",cp_feat3.shape)\n#             cp_feat4 = sigmoid(beta*np.dot(train_data_curr,ew[3].cpu().T).reshape(-1,1,1,1,num_neuron))\n#             print(\"cp_feat4:\",cp_feat4.shape)\n            cp_feat = cp_feat1 * cp_feat2 * cp_feat3 #* cp_feat4\n#             print(\"cp_feat:\",cp_feat.shape)\n            cp_feat_vec = cp_feat.reshape((len(cp_feat),-1))\n#             print(\"cp_feat_vec:\",cp_feat_vec.shape)\n\n            clf = LogisticRegression(C=0.03, fit_intercept=False,max_iter=1000, penalty=\"l1\", solver='liblinear')\n            clf.fit(2*cp_feat_vec, train_labels_curr)\n            value_wts  = clf.decision_function(np.eye(num_neuron**num_layer)).reshape(1,num_neuron,num_neuron,num_neuron)\n#             print(\"value_wts:\",value_wts.shape)\n            A= DLGN_obj.value_layers.detach()\n            A[:] = torch.Tensor(value_wts)\n\n            train_preds =DLGN_obj(torch.Tensor(train_data_curr).to(device)).reshape((-1,1))\n            criterion = nn.CrossEntropyLoss()\n            outputs = torch.cat((-1*train_preds,train_preds), dim=1)\n            targets = torch.tensor(train_labels_curr, dtype=torch.int64).to(device)\n            train_loss = criterion(outputs, targets)\n            print(\"Loss after updating value_net at epoch\", epoch, \" is \", train_loss)\t\t\t\n            print(\"Total path abs value\", torch.abs(DLGN_obj.value_layers.cpu().detach()).sum().numpy())\n    # \t\tif epoch in saved_epochs:\n    # \t\t\tDLGN_obj_copy = deepcopy(DLGN_obj)\n    # \t\t\tDLGN_obj_copy.to(torch.device('cpu'))\n    # \t\t\tDLGN_obj_store.append(DLGN_obj_copy)\n\n        for batch_start in range(0,len(train_data_curr),batch_size):\n            if (batch_start+batch_size)>len(train_data_curr):\n                break\n            optimizer.zero_grad()\n            inputs = train_data_torch[batch_start:batch_start+batch_size]\n            targets = train_labels_torch[batch_start:batch_start+batch_size].reshape(batch_size)\n            criterion = nn.CrossEntropyLoss()\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            preds = DLGN_obj(inputs).reshape(-1,1)\n            # preds_clone = preds.detach().clone().cpu().numpy()[:,0]\n            # targets_clone = targets.detach().clone().cpu().numpy()\n            # coeff = (0.5-targets_clone)/(sigmoid(2*preds_clone)-targets_clone)\n            # print(coeff.shape)\n\n            # print(coeff.min())\n            # print(coeff.mean())\n            # print(coeff.max())\n            outputs = torch.cat((-1*preds, preds), dim=1)\n            loss = criterion(outputs, targets)\n            # loss = loss*torch.tensor(coeff, device=device)\t\n            # loss = loss.mean()\t\t\n            loss.backward()\n            for name,param in DLGN_obj.named_parameters():\n                if \"val\" in name:\n                    param.grad *= 0.0\n                if \"gat\" in name:\n                    param.grad *= 1.0\n            optimizer.step()\n\n        train_preds =DLGN_obj(torch.Tensor(train_data_curr).to(device)).reshape(-1,1)\n        criterion = nn.CrossEntropyLoss()\n        outputs = torch.cat((-1*train_preds,train_preds), dim=1)\n        targets = torch.tensor(train_labels_curr, dtype=torch.int64).to(device)\n        train_loss = criterion(outputs, targets)\n        if epoch%5 == 0:\n            print(\"Loss after updating at epoch \", epoch, \" is \", train_loss)\n            test_preds =DLGN_obj(test_data_torch.to(device)).reshape(-1,1)\n            test_preds = test_preds.detach().cpu().numpy()\n            print(\"Test error=\",np.sum(test_labels_curr != (np.sign(test_preds[:,0])+1)//2 ))\n        if train_loss < 0.005:\n            break\n        if np.isnan(train_loss.detach().cpu().numpy()):\n            break\n\n        losses.append(train_loss.cpu().detach().clone().numpy())\n        inputs = vali_data_torch.to(device)\n        targets = vali_labels_torch.to(device)\n        preds =DLGN_obj(inputs).reshape(-1,1)\n        vali_preds = torch.cat((-1*preds, preds), dim=1)\n        vali_preds = torch.argmax(vali_preds, dim=1)\n        vali_error= torch.sum(targets!=vali_preds)\n        if vali_error < best_vali_error:\n            DLGN_obj_return = deepcopy(DLGN_obj)\n            best_vali_error = vali_error\n    plt.figure()\n    plt.title(\"DLGN loss vs epoch\")\n    plt.plot(losses)\n    # \tif not os.path.exists('figures'):\n    # \t\tos.mkdir('figures')\n\n    # \tfilename = 'figures/'+filename_suffix +'.pdf'\n    # \tplt.savefig(filename)\n    DLGN_obj_return.to(torch.device('cpu'))\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # device = torch.device('cpu')\n    return train_losses, DLGN_obj_return, DLGN_obj_store, losses, debug_models","metadata":{"id":"Ncr5k6koMbD_","execution":{"iopub.status.busy":"2024-03-13T20:24:04.758379Z","iopub.execute_input":"2024-03-13T20:24:04.759013Z","iopub.status.idle":"2024-03-13T20:24:04.795566Z","shell.execute_reply.started":"2024-03-13T20:24:04.758977Z","shell.execute_reply":"2024-03-13T20:24:04.794760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data_adult(data_path):\n    # Read the data into a DataFrame\n    columns = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n        \"hours-per-week\", \"native-country\", \"income\"\n    ]\n    df = pd.read_csv(data_path, names=columns, na_values=[\" ?\"])\n\n    # Drop rows with missing values\n    df.dropna(inplace=True)\n\n    # Convert categorical features using Label Encoding\n    categorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n    label_encoders = {}\n    for col in categorical_columns:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n        label_encoders[col] = le\n\n    # Encode the target variable\n    df[\"income\"] = df[\"income\"].apply(lambda x: 1 if x == \" >50K\" else 0)\n\n    return df\n\ndef preprocess_data_bank_marketing(data):\n    # Convert categorical features using Label Encoding\n    label_encoders = {}\n    for col in data.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        data[col] = le.fit_transform(data[col])\n        label_encoders[col] = le\n\n    return data\n\ndef preprocess_data_credit_card_defaults(data):\n    # Convert categorical features using one-hot encoding\n    data = pd.get_dummies(data, columns=[\"SEX\", \"EDUCATION\", \"MARRIAGE\"], drop_first=True)\n\n    # Standardize numerical features\n    scaler = StandardScaler()\n    data[[\"LIMIT_BAL\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"BILL_AMT1\",\n          \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\",\n          \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]] = scaler.fit_transform(\n        data[[\"LIMIT_BAL\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"BILL_AMT1\",\n               \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\",\n               \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]])\n\n    return data\n\n\ndef fetch_ADULT(data_dir=\"./ADULT_DATA\"):\n    print(\"---------------------ADULT--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n        \n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/2/adult.zip\"\n    zip_file_path = os.path.join(data_dir, \"adult.zip\")\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n\n    # Preprocess the data\n    train_data_path = os.path.join(data_dir, \"adult.data\")\n#     test_data_path = os.path.join(data_dir, \"adult.test\")\n   \n    df_train = preprocess_data_adult(train_data_path)\n#     df_test = preprocess_data_adult(test_data_path)\n\n    # Split the data into train, validation, and test sets\n    X = df_train.drop(\"income\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df_train[\"income\"]\n    \n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n#     X_test = df_test.drop(\"income\", axis=1)\n#     y_test = df_test[\"income\"]\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents using shutil.rmtree()\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train, X_valid=X_val.astype('float32'), y_valid=y_val, X_test=X_test.astype('float32'), y_test=y_test\n    )\n\ndef fetch_bank_marketing(data_dir=\"./BANK\"):\n    print(\"---------------------BANK--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\"\n    zip_file_path = os.path.join(data_dir, \"bank_marketing.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n    \n    zip_file_path_bank_add = os.path.join(data_dir, \"bank-additional.zip\")\n    with zipfile.ZipFile(zip_file_path_bank_add, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n\n    # Get the extracted directory path\n    extracted_dir = os.path.join(data_dir, \"bank-additional\")\n\n    # Read the dataset\n    data = pd.read_csv(os.path.join(extracted_dir, \"bank-additional-full.csv\"), sep=';')\n\n    # Preprocess the data\n    data = preprocess_data_bank_marketing(data)\n\n    # Split the data into train, validation, and test sets\n    X = data.drop(\"y\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"y\"]\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,X_test=X_test.astype('float32'), y_test=y_test, X_valid = X_val.astype('float32'), y_valid = y_val\n    )\n\ndef fetch_credit_card_defaults(data_dir=\"./CREDIT\"):\n    print(\"---------------------CREDIT--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    !pip install xlrd\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\"\n    zip_file_path = os.path.join(data_dir, \"credit_card_defaults.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n\n#     # Get the extracted directory path\n#     extracted_dir = os.path.join(data_dir, \"default+of+credit+card+clients\")\n\n    # Read the dataset\n    data = pd.read_excel(os.path.join(data_dir, \"default of credit card clients.xls\"), skiprows=1)\n\n    # Preprocess the data\n    data = preprocess_data_credit_card_defaults(data)\n\n    # Split the data into train, validation, and test sets\n    X = data.drop(\"default payment next month\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"default payment next month\"]\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train, X_valid=X_val.astype('float32'), y_valid=y_val , X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_gamma_telescope(data_dir=\"./TELESCOPE\"):\n    print(\"---------------------TELESCOPE--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\"\n    zip_file_path = os.path.join(data_dir, \"magic_gamma_telescope.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n    \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"magic04.data\")\n    columns = [\n        \"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\",\n        \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"\n    ]\n    data = pd.read_csv(data_path, header=None, names=columns)\n    \n    # Convert the class labels to binary format (g = gamma, h = hadron)\n    data[\"class\"] = data[\"class\"].map({\"g\": 1, \"h\": 0})\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_rice_dataset(data_dir=\"./RICE\"):\n    print(\"---------------------RICE--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/545/rice+cammeo+and+osmancik.zip\"\n    zip_file_path = os.path.join(data_dir, \"rice_dataset.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    arff_file_name = os.path.join(data_dir, \"Rice_Cammeo_Osmancik.arff\")\n\n    \n    # Load the ARFF file using SciPy\n    data, meta = arff.loadarff(arff_file_name)\n    \n    df = pd.DataFrame(data)\n    print(\"df\",df)\n    df[\"Class\"] = df[\"Class\"].map({b'Cammeo': 1, b'Osmancik': 0})\n    \n    # Split the data into features (X) and target (y)\n    X = df.drop(\"Class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[\"Class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_german_credit_data(data_dir=\"./GERMAN\"):\n    print(\"---------------------GERMAN--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"http://archive.ics.uci.edu/static/public/144/statlog+german+credit+data.zip\"\n    zip_file_path = os.path.join(data_dir, \"german_credit_data.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"german.data\")\n\n    columns = [\n        \"checking_account_status\", \"duration_months\", \"credit_history\", \"purpose\",\n        \"credit_amount\", \"savings_account_bonds\", \"employment\", \"installment_rate\",\n        \"personal_status_sex\", \"other_debtors_guarantors\", \"present_residence\",\n        \"property\", \"age\", \"other_installment_plans\", \"housing\", \"existing_credits\",\n        \"job\", \"num_dependents\", \"own_telephone\", \"foreign_worker\", \"class\"\n    ]\n    data = pd.read_csv(data_path, sep=' ', header=None, names=columns)\n    \n    # Convert the class labels to binary format (1 = Good, 2 = Bad)\n    data[\"class\"] = data[\"class\"].map({1: 1, 2: 0})\n    \n    # Handle null values (replace with appropriate values)\n    data.fillna(method='ffill', inplace=True)  # Forward fill\n    \n    # Convert categorical variables to dummy variables\n    categorical_columns = [\n        \"checking_account_status\", \"credit_history\", \"purpose\", \"savings_account_bonds\",\n        \"employment\", \"personal_status_sex\", \"other_debtors_guarantors\", \"property\",\n        \"other_installment_plans\", \"housing\", \"job\", \"own_telephone\", \"foreign_worker\"\n    ]\n    data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_spambase_dataset(data_dir=\"./SPAM\"):\n    print(\"---------------------SPAM--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"http://archive.ics.uci.edu/static/public/94/spambase.zip\"\n    zip_file_path = os.path.join(data_dir, \"spambase.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"spambase.data\")\n\n    columns = [\n        f\"f{i}\" for i in range(57)\n    ] + [\"spam\"]\n    data = pd.read_csv(data_path, header=None, names=columns)\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"spam\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"spam\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_accelerometer_gyro_dataset(data_dir=\"./GYRO\"):\n    print(\"---------------------GYRO--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/755/accelerometer+gyro+mobile+phone+dataset.zip\"\n    zip_file_path = os.path.join(data_dir, \"accelerometer_gyro_dataset.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"accelerometer_gyro_mobile_phone_dataset.csv\")\n    \n    data = pd.read_csv(data_path)\n    \n    # Convert categorical column to numeric (e.g., label encoding)\n    data[\"timestamp\"] = data[\"timestamp\"].astype(\"category\").cat.codes\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"Activity\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"Activity\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir)\n    \n    return data_splits\n\ndef fetch_swarm_behaviour(data_dir=\"./SWARM\"):\n    print(\"---------------------SWARM--------------------------------------\")\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # URL of the dataset zip file\n    url = \"https://archive.ics.uci.edu/static/public/524/swarm+behaviour.zip\"\n    zip_file_path = os.path.join(data_dir, \"swarm_behaviour.zip\")\n\n    # Download the zip file\n    urllib.request.urlretrieve(url, zip_file_path)\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(data_dir)\n        \n    # Load the data from CSV\n    data_path = os.path.join(data_dir, \"Swarm Behavior Data/Grouped.csv\")\n    \n    data = pd.read_csv(data_path)\n    \n    # Split the data into features (X) and target (y)\n    X = data.drop(\"Class\", axis=1)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = data[\"Class\"]\n    \n    # Split the data into train, test, and validation sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_valid = (y_valid.values.reshape(-1) == 1).astype('int64')\n    \n    # Create a dictionary to store the data splits\n    data_splits = {\n        \"X_train\": X_train.astype('float32'), \"y_train\": y_train,\n        \"X_valid\": X_valid.astype('float32'), \"y_valid\": y_valid,\n        \"X_test\": X_test.astype('float32'), \"y_test\": y_test\n    }\n    \n    # Remove the zip file\n    os.remove(zip_file_path)\n    # Remove the extracted directory and its contents\n    shutil.rmtree(data_dir) \n    return data_splits\n\n\ndef fetch_openml_credit_data(data_dir=\"./OpenML_Credit\"):\n    print(\"---------------------OpenML_Credit DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103185/credit.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"credit.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n    df[last_column] = df[last_column].astype(int)\n    \n#     print(\"df\",df)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_electricity_data(data_dir=\"./OpenML_Electricity\"):\n    print(\"---------------------OpenML_Electricity DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103245/electricity.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"electricity.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n    df[last_column] = df[last_column].map({b'DOWN': 0, b'UP': 1})\n    \n#     print(\"df\",df)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_covertype_data(data_dir=\"./OpenML_Covertype\"):\n    print(\"---------------------OpenML_Covertype DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103246/covertype.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"covertype.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n    df[last_column] = df[last_column].astype(int)\n    \n#     print(\"df\",df)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_pol_data(data_dir=\"./OpenML_Pol\"):\n    print(\"---------------------OpenML_Pol DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103247/pol.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"pol.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n#     print(\"df\",df)\n    \n    df[last_column] = df[last_column].map({b'N':0,b'P':1})\n    \n    \n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_house_16H_data(data_dir=\"./OpenML_House_16H\"):\n    print(\"---------------------OpenML_House_16H DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103248/house_16H.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"house_16H.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n#     print(\"df\",df)\n    df[last_column] = df[last_column].map({b'N':0,b'P':1})\n    \n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_MiniBooNE_data(data_dir=\"./OpenML_MiniBooNE\"):\n    print(\"---------------------OpenML_MiniBooNE DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103253/MiniBooNE.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"MiniBooNE.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n#     print(\"df\",df)\n    \n    df[last_column] = df[last_column].map({b'False':0,b'True':1})\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_eye_movements_data(data_dir=\"./OpenML_Eye_movements\"):\n    print(\"---------------------OpenML_Eye_movements DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22103255/eye_movements.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"eye_movements.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n\n#     print(\"df\",df)\n    df[last_column] = df[last_column].astype(int)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_Diabetes130US_data(data_dir=\"./OpenML_Diabetes130US\"):\n    print(\"---------------------OpenML_Diabetes130US DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22111908/Diabetes130US.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"Diabetes130US.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n#     print(\"df\",df)\n    df[last_column] = df[last_column].astype(int)\n    \n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_jannis_data(data_dir=\"./OpenML_Jannis\"):\n    print(\"---------------------OpenML_Jannis DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22111907/jannis.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"jannis.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n#     print(\"df\",df)\n\n    df[last_column] = df[last_column].astype(int)\n\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_Bioresponse_data(data_dir=\"./OpenML_Bioresponse\"):\n    print(\"---------------------OpenML_Bioresponse DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22111905/Bioresponse.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"Bioresponse.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n#     print(\"df\",df)\n\n    df[last_column] = df[last_column].astype(int)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_california_data(data_dir=\"./OpenML_California\"):\n    print(\"---------------------OpenML_California DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22111914/california.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"california.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n#     print(\"df\",df)\n\n    df[last_column] = df[last_column].astype(int)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\ndef fetch_openml_heloc_data(data_dir=\"./OpenML_Heloc\"):\n    print(\"---------------------OpenML_Heloc DATASET--------------------------------------\")\n    # Create the data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_url = \"https://api.openml.org/data/v1/download/22111912/heloc.arff\"\n    # Download the ARFF file\n    arff_file_path = os.path.join(data_dir, \"heloc.arff\")\n    urllib.request.urlretrieve(data_url, arff_file_path)\n\n    # Load ARFF file into DataFrame\n    data, meta = arff.loadarff(arff_file_path)\n    df = pd.DataFrame(data)\n    # Convert target variable to int\n    last_column = df.columns[-1]\n#     print(\"df\",df)\n\n    df[last_column] = df[last_column].astype(int)\n\n    # Split the data into train, validation, and test sets\n    X = df.drop(last_column, axis=1)  # Assuming \"SeriousDlqin2yrs\" is the target variable\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = df[last_column]\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n\n#     y_train = y_train.astype('int64')\n#     y_test = y_test.astype('int64')\n#     y_val = y_val.astype('int64')\n\n    y_train = (y_train.values.reshape(-1) == 1).astype('int64')\n    y_test = (y_test.values.reshape(-1) == 1).astype('int64')\n    y_val = (y_val.values.reshape(-1) == 1).astype('int64')\n\n    # Remove the ARFF file\n    os.remove(arff_file_path)\n\n    # Remove the data directory\n    shutil.rmtree(data_dir)\n\n    return dict(\n        X_train=X_train.astype('float32'), y_train=y_train,\n        X_valid=X_val.astype('float32'), y_valid=y_val,\n        X_test=X_test.astype('float32'), y_test=y_test\n    )\n\n\n#**class Dataset:**\n\nREAL_DATASETS = {\n    ####### 10 latest UCI datasets ########\n    'ADULT': fetch_ADULT,\n    'bank_marketing': fetch_bank_marketing,\n    'credit_card_defaults': fetch_credit_card_defaults,\n    'gamma_telescope': fetch_gamma_telescope,\n    'rice_dataset': fetch_rice_dataset,\n    'german_credit_data': fetch_german_credit_data,\n    'spambase_dataset': fetch_spambase_dataset,\n    'accelerometer_gyro_dataset': fetch_accelerometer_gyro_dataset,\n    'swarm_behaviour': fetch_swarm_behaviour,\n    ######## OpenML Tabular Datasets ##########\n    'OpenML_Credit': fetch_openml_credit_data,\n    'OpenML_Electricity': fetch_openml_electricity_data,\n    'OpenML_Covertype': fetch_openml_covertype_data,\n    'OpenML_Pol': fetch_openml_pol_data,\n    'OpenML_House_16H': fetch_openml_house_16H_data,\n    'OpenML_MiniBooNE': fetch_openml_MiniBooNE_data,\n    'OpenML_Eye_movements': fetch_openml_eye_movements_data,\n    'OpenML_Diabetes130US': fetch_openml_Diabetes130US_data,\n    'OpenML_Jannis': fetch_openml_jannis_data,\n    'OpenML_Bioresponse': fetch_openml_Bioresponse_data,\n    'OpenML_California': fetch_openml_california_data,\n    'OpenML_Heloc': fetch_openml_heloc_data\n}\n\nclass Dataset:\n    def __init__(self, dataset, data_path='./DATA', normalize=False, normalize_target=False, quantile_transform=False, quantile_noise=1e-3, in_features=None, out_features=None, flatten=False, **kwargs):\n        \"\"\"\n        Dataset is a dataclass that contains all training and evaluation data required for an experiment\n        :param dataset: a pre-defined dataset name (see DATASETS) or a custom dataset\n            Your dataset should be at (or will be downloaded into) {data_path}/{dataset}\n        :param data_path: a shared data folder path where the dataset is stored (or will be downloaded into)\n        :param normalize: standardize features by removing the mean and scaling to unit variance\n        :param quantile_transform: whether tranform the feature distributions into normals, using a quantile transform\n        :param quantile_noise: magnitude of the quantile noise\n        :param in_features: which features to use as inputs\n        :param out_features: which features to reconstruct as output\n        :param flatten: whether flattening instances to vectors\n        :param kwargs: depending on the dataset, you may select train size, test size or other params\n        \"\"\"\n\n        if dataset in REAL_DATASETS:\n            data_dict = REAL_DATASETS[dataset](Path(data_path) / dataset, **kwargs)\n\n            self.X_train = data_dict['X_train']\n            self.y_train = data_dict['y_train']\n            self.X_valid = data_dict['X_valid']\n            self.y_valid = data_dict['y_valid']\n            self.X_test = data_dict['X_test']\n            self.y_test = data_dict['y_test']\n\n            if flatten:\n                self.X_train, self.X_valid, self.X_test = self.X_train.reshape(len(self.X_train), -1), self.X_valid.reshape(len(self.X_valid), -1), self.X_test.reshape(len(self.X_test), -1)\n\n            if normalize:\n\n                print(\"Normalize dataset\")\n                axis = [0] + [i + 2 for i in range(self.X_train.ndim - 2)]\n                self.mean = np.mean(self.X_train, axis=tuple(axis), dtype=np.float32)\n                self.std = np.std(self.X_train, axis=tuple(axis), dtype=np.float32)\n\n                # if constants, set std to 1\n                self.std[self.std == 0.] = 1.\n\n                if dataset not in ['ALOI']:\n                    self.X_train = (self.X_train - self.mean) / self.std\n                    self.X_valid = (self.X_valid - self.mean) / self.std\n                    self.X_test = (self.X_test - self.mean) / self.std\n\n            if quantile_transform:\n                quantile_train = np.copy(self.X_train)\n                if quantile_noise:\n                    stds = np.std(quantile_train, axis=0, keepdims=True)\n                    noise_std = quantile_noise / np.maximum(stds, quantile_noise)\n                    quantile_train += noise_std * np.random.randn(*quantile_train.shape)\n\n                qt = QuantileTransformer(output_distribution='normal').fit(quantile_train)\n                self.X_train = qt.transform(self.X_train)\n                self.X_valid = qt.transform(self.X_valid)\n                self.X_test = qt.transform(self.X_test)\n\n            if normalize_target:\n\n                print(\"Normalize target value\")\n                self.mean_y = np.mean(self.y_train, axis=0, dtype=np.float32)\n                self.std_y = np.std(self.y_train, axis=0, dtype=np.float32)\n\n                # if constants, set std to 1\n                if self.std_y == 0.:\n                    self.std_y = 1.\n\n                self.y_train = (self.y_train - self.mean_y) / self.std_y\n                self.y_valid = (self.y_valid - self.mean_y) / self.std_y\n                self.y_test = (self.y_test - self.mean_y) / self.std_y\n\n            if in_features is not None:\n                self.X_train_in, self.X_valid_in, self.X_test_in = self.X_train[:, in_features], self.X_valid[:, in_features], self.X_test[:, in_features]\n\n            if out_features is not None:\n                self.X_train_out, self.X_valid_out, self.X_test_out = self.X_train[:, out_features], self.X_valid[:, out_features], self.X_test[:, out_features]\n\n        elif dataset in TOY_DATASETS:\n            data_dict = toy_dataset(distr=dataset, **kwargs)\n\n            self.X = data_dict['X']\n            self.Y = data_dict['Y']\n            if 'labels' in data_dict:\n                self.labels = data_dict['labels']\n\n        self.data_path = data_path\n        self.dataset = dataset\n\nclass TorchDataset(torch.utils.data.Dataset):\n\n    def __init__(self, *data, **options):\n        \n        n_data = len(data)\n        if n_data == 0:\n            raise ValueError(\"At least one set required as input\")\n\n        self.data = data\n        means = options.pop('means', None)\n        stds = options.pop('stds', None)\n        self.transform = options.pop('transform', None)\n        self.test = options.pop('test', False)\n        \n        if options:\n            raise TypeError(\"Invalid parameters passed: %s\" % str(options))\n         \n        if means is not None:\n            assert stds is not None, \"must specify both <means> and <stds>\"\n\n            self.normalize = lambda data: [(d - m) / s for d, m, s in zip(data, means, stds)]\n\n        else:\n            self.normalize = lambda data: data\n\n    def __len__(self):\n        return len(self.data[0])\n\n    def __getitem__(self, idx):\n        data = self.normalize([s[idx] for s in self.data])\n        if self.transform:\n\n            if self.test:\n                data = sum([[self.transform.test_transform(d)] * 2 for d in data], [])\n            else:\n                data = sum([self.transform(d) for d in data], [])\n            \n        return data","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:04.800626Z","iopub.execute_input":"2024-03-13T20:24:04.800941Z","iopub.status.idle":"2024-03-13T20:24:05.128259Z","shell.execute_reply.started":"2024-03-13T20:24:04.800915Z","shell.execute_reply":"2024-03-13T20:24:05.127218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training a DLGN model**","metadata":{"id":"PpN8Yby7Fllw"}},{"cell_type":"code","source":"args =  Args()\n\nnum_layer = args.numlayer\nnum_neuron = args.numnodes\nbeta = args.beta\nlr=args.lr\n\nsaved_epochs = list(range(0,300,10)) + list(range(300,10001,50))\nupdate_value_epochs = list(range(0,10001,100))# \n\nseed = 365\nno_of_batches=10 #[1,10,100]\nweight_decay=0.0\nnum_hidden_nodes=[num_neuron]*num_layer\n\n\noptimizer_name ='Adam'\nmodep='pwc' \noutput_dim=1\n\nweight_decay=0.0\n\n# DATA_NAME=[\"OpenML_Credit\",\"OpenML_Electricity\",\"OpenML_Pol\",\"OpenML_House_16H\",\"OpenML_MiniBooNE\",\"OpenML_Eye_movements\",\"OpenML_Diabetes130US\",\"OpenML_Jannis\",\"OpenML_Bioresponse\",\"OpenML_California\",\"OpenML_Heloc\"]#\"OpenML_Covertype\"]#,\"bank_marketing\",\"credit_card_defaults\",\"gamma_telescope\",\"rice_dataset\",\"german_credit_data\",\"spambase_dataset\",\"accelerometer_gyro_dataset\",\"swarm_behaviour\"]#,\"HIGGS\"]\nDATA_NAME = [\"OpenML_Covertype\"]\nfor data_name in DATA_NAME: \n    data = Dataset(data_name,normalize=True)\n    print('classes', np.unique(data.y_test))\n    set_npseed(seed)\n    set_torchseed(seed)\n    input_dim=data.X_train.shape[1]\n\n    set_torchseed(6675)\n    \n    train_data = data.X_train\n    train_data_labels =  data.y_train\n\n    vali_data = data.X_valid\n    vali_data_labels = data.y_valid\n\n    test_data = data.X_test\n    test_data_labels = data.y_test  \n    \n    print(\"train_data:\",train_data.shape,\"vali_data:\",vali_data.shape,\"test_data:\",test_data.shape)\n\n    print(\"---\" * 30)\n    set_torchseed(365)\n    # set_torchseed(5612)\n    DLGN_init= DLGN_FC(input_dim=input_dim, output_dim=1, num_hidden_nodes=num_hidden_nodes, beta=beta)\n\n    train_parameter_masks=dict()\n    \n    for name,parameter in DLGN_init.named_parameters():\n        if \"val\" in name:\n            train_parameter_masks[name]=torch.ones_like(parameter)# Updating all value network layers\n        if \"gat\" in name:\n            train_parameter_masks[name]=torch.ones_like(parameter)\n        train_parameter_masks[name].to(device)\n\n\n        \n\n\n\n\n\n\n    set_torchseed(8)\n    train_losses, DLGN_obj_final, DLGN_obj_store, losses , debug_models= train_dlgn(train_data_curr=train_data,\n                                                vali_data_curr=vali_data,\n                                                test_data_curr=test_data,\n                                                train_labels_curr=train_data_labels,\n                                                vali_labels_curr=vali_data_labels,\n                                                test_labels_curr=test_data_labels,\n                                                DLGN_obj=deepcopy(DLGN_init),\n                                                parameter_mask=train_parameter_masks,\n                                                )\n\n\n    torch.cuda.empty_cache() \n    losses=np.array(losses)\n    \n    \n    device=torch.device('cpu')\n    train_preds =DLGN_obj_final(torch.Tensor(train_data).to(device)).reshape(-1,1)\n    criterion = nn.CrossEntropyLoss()\n    outputs = torch.cat((-1*train_preds,train_preds), dim=1)\n    targets = torch.tensor(train_data_labels, dtype=torch.int64)\n    train_loss = criterion(outputs, targets)\n    train_preds = train_preds.detach().numpy()\n    # filename = 'outputs/'+filename_suffix+'.txt'\n    # original_stdout = sys.stdout\n    Train_error = np.sum(train_data_labels != (np.sign(train_preds[:,0])+1)//2)\n    Num_train_data = len(train_data_labels)\n    print(\"Train error=\",Train_error)\n    print(\"Num_train_data=\",Num_train_data)\n    print(\"Train_acc:\",1-Train_error/Num_train_data)\n    \n    test_preds =DLGN_obj_final(torch.Tensor(test_data)).reshape(-1,1)\n    test_preds = test_preds.detach().numpy()\n    # filename = 'outputs/'+filename_suffix+'.txt'\n    # original_stdout = sys.stdout\n    # with open(filename,'a') as f:\n    #     sys.stdout = f\n    #     print(\"Test error=\",np.sum(test_data_labels != (np.sign(test_preds[:,0])+1)//2 ))\n    #     print(\"Num_test_data=\",len(test_data_labels))\n    #     sys.stdout = original_stdout\n\n    Test_error = np.sum(test_data_labels != (np.sign(test_preds[:,0])+1)//2)\n    Num_test_data = len(test_data_labels)\n    print(\"Test error=\",Test_error)\n    print(\"Num_test_data=\",Num_test_data)\n    print(\"Test_acc:\",1-Test_error/Num_test_data)\n\n# print(DLGN_obj_store[-1].beta)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-13T20:24:05.130035Z","iopub.execute_input":"2024-03-13T20:24:05.130496Z"},"trusted":true},"execution_count":null,"outputs":[]}]}